# GPT-4o Benchmark Results

## Table of Contents

- [Introduction](#introduction)
- [Conducting an experiment](#conducting-an-experiment)
    - [Code Generation Experiment](#calculated-scores-for-code-generation-experiment)
    - [Code Translation Experiment](#calculated-scores-for-code-translation-experiment)
    - [Code Documentation Experiment](#calculated-scores-for-code-documentation-experiment)
    - [Large Context Instructions Following](#calculated-scores-for-large-context-instructions-following)
- [Total Scores](#calculated-total-scores)
- [Conclusions](#conclusions)

## Introduction

The benchmark for GPT-4o was conducted on a limited dataset and compared with the corresponding subsets of Claude Opus,
Gemini Pro 1.5, and GPT-4 Turbo (09-04-2024) results.

The benchmark quality should not be significantly affected because previous research has shown that these models are
tolerant to increases in the datasets' code complexity.
Therefore, the benchmark was reduced to evaluate the model's behavior across varying dataset sizes, from low to high,
which was the most impactful factor based on previous research.

## Conducting an experiment

### Calculated Scores for Code Generation Experiment

| Model       | Avg Attempt | Avg Accuracy | Avg Completeness | Avg Time    | Avg token     | Avg Tokens/second | Score  |
|-------------|-------------|--------------|------------------|-------------|---------------|-------------------|--------|
| Claude Opus | **1.0000**  | **3.4286**   | 3.1429           | 58.8112     | **1808.9524** | 30.7587           | 0.8057 |
| Gemini Pro  | 1.1429      | 2.8571       | 2.5714           | 22.2382     | 714.7619      | 32.1412           | 0.6864 |
| GPT-4 Turbo | 1.3750      | 3.0000       | 2.5714           | 52.7559     | 750.4762      | 14.2255           | 0.6608 |
| GPT-4o      | 2.1429      | 3.1429       | **3.7143**       | **21.0028** | 1330.9524     | **63.3702**       | 0.8286 |

**Observations:**

In this experiment, GPT-4o demonstrated very good completeness, as the model often tried to meet all the conditions of
the task. However, it sometimes did so with errors, which led to a reduction in its accuracy score.

### Calculated Scores for Code Translation Experiment

| Model       | Avg Attempt | Avg Accuracy | Avg Completeness | Avg Time    | Avg token     | Avg Tokens/second | Score  |
|-------------|-------------|--------------|------------------|-------------|---------------|-------------------|--------|
| Claude Opus | **1.1429**  | **3.5714**   | 3.1429           | 79.1715     | **2714.5714** | 34.2872           | 0.8288 |
| Gemini Pro  | 1.7143      | **3.5714**   | 3.1429           | 58.5983     | 2030.3333     | 34.6483           | 0.8009 |
| GPT-4 Turbo | 2.1429      | 3.2857       | 3.0000           | 95.4680     | 1658.5714     | 17.3731           | 0.7041 |
| GPT-4o      | 1.5714      | 3.0000       | **3.7143**       | **50.6905** | 2694.9048     | **53.1639**       | 0.8429 |

**Observations:** Here again, GPT-4o shows much better completeness. It should be noted that although Claude Opus
generates more tokens, GPT-4o produces a greater number of characters with fewer tokens.

### Calculated Scores for Code Documentation Experiment

| Model       | Avg Attempt | Avg Accuracy | Avg Completeness | Avg Time    | Avg token     | Avg Tokens/second | Score  |
|-------------|-------------|--------------|------------------|-------------|---------------|-------------------|--------|
| Claude Opus | **1.0000**  | 3.6667       | **3.8889**       | 56.5806     | 1545.4444     | 27.3141           | 0.9042 |
| Gemini Pro  | 1.6667      | 3.2222       | 3.4444           | 53.1513     | 1920.1111*    | 36.1254           | 0.7977 |
| GPT-4 Turbo | 1.7778      | **3.8889**   | 3.7778           | 44.6062     | 772.6667      | 17.3219           | 0.8586 |
| GPT-4o      | 1.5000      | 3.6250       | 3.7500           | **28.7832** | **1616.4444** | **56.1594**       | 0.9125 |

\* - Gemini Pro had a bug that resulted in the maximum number of tokens being generated in some tests.

**Observations:** In the documentation, GPT-4o did not show outstanding results, but it also did not perform
significantly worse than the others. The model generated many tokens, but they did not always fully solve the task at
hand.

### Calculated Scores for Large Context Instructions Following

| Model           | Avg Accuracy | Avg Completeness | Score  |
|-----------------|--------------|------------------|--------|
| **Claude Opus** | 3            | **4**            | 0.8750 |
| **Gemini Pro**  | 3            | 3.5              | 0.8125 |
| **GPT-4 Turbo** | 3            | 3.5              | 0.8125 |
| **GPT-4o**      | **3.5**      | 3.5              | 0.8750 |

**Observations:** In the Large Context Instructions Following, GPT-4o was unable to achieve the highest result in completeness, despite
several attempts. On the other hand, this indicates the stability of the model's responses. The final results were on
par with Claude Opus.

## Calculated Total scores

| Model       | Code Translation | Generation | Document | LCIF   | Total Score |
|-------------|------------------|------------|----------|--------|-------------|
| Claude Opus | 0.829            | 0.806      | 0.904    | 0.875  | 0.853       |
| Gemini Pro  | 0.801            | 0.686      | 0.798    | 0.8125 | 0.774       |
| GPT-4 Turbo | 0.704            | 0.661      | 0.859    | 0.8125 | 0.759       |
| GPT-4o      | 0.843            | 0.829      | 0.913    | 0.875  | 0.865       |

## Conclusions

We observe an improvement in the performance of the GPT-4o model compared to the previous version in terms of accuracy
and completeness. Notably, the model has started generating more output tokens, reducing the number of todos, thereby
increasing completeness, and it is also evident that the model more frequently delivers better results on the first
attempt.

It is also important to note the speed of query execution; on average, the new model is four times faster than GPT-4
Turbo and twice as fast as the other models. This result may change over time, as the model is not yet under as heavy a
load as GPT-4 Turbo.

According to our benchmark, GPT-4o scored the highest in most categories, although in individual experiments it was
outperformed by other models in terms of accuracy and completeness.

The final result achieved a high value in tokens per second (on average 1.5-3.5 times more than other models).

GPT-4o will be a good choice to replace the GPT-4 Turbo model, but overall, the results of our previous research remain
valid except in the categories of speed and completeness - now GPT-4o leads in these aspects.

<p style="text-align: center;">
    Â© 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>