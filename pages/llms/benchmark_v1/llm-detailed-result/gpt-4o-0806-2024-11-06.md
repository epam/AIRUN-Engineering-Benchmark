# GPT-4o 0806 (2024-11-06)

## Introduction

This article presents an analysis of various code-related tasks performed by the GPT-4o (0806) language model. We'll examine the model's performance across different categories, like translation, generation, and documentation.

## Evaluation Summary

The following table provides detailed information on each experiment conducted. It includes the type of experiment, category, programming language, dataset used, complexity, input and output sizes, time taken, and accuracy and completeness

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Reasons | Output | Time   | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|---------|--------|--------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3269  | 0       | 2553   | 51.12  | 4        | 2.65         |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2343  | 0       | 4167   | 62.00  | 4        | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5413  | 0       | 1609   | 18.64  | 4        | 3            |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3258  | 0       | 2463   | 45.61  | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2275  | 0       | 1553   | 24.78  | 3.38     | 2.8          |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1273  | 0       | 1336   | 17.22  | 2.35     | 2.73         |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1402  | 0       | 1764   | 26.33  | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 379   | 0       | 530    | 6.86   | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 199   | 0       | 1117   | 24.04  | 4        | 4            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 188   | 0       | 619    | 15.05  | 3.15     | 3.99         |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1230  | 0       | 947    | 12.29  | 2.73     | 2.32         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3226  | 0       | 1806   | 32.97  | 4        | 4            |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5389  | 0       | 1076   | 11.98  | 2.04     | 1.9          |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 15764 | 0       | 1204   | 18.69  | 4        | 1.88         |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1223  | 0       | 729    | 9.34   | 4        | 3.99         |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3219  | 0       | 685    | 12.50  | 3.68     | 4            |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5382  | 0       | 853    | 9.86   | 2.91     | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1347  | 0       | 1067   | 11.08  | 4        | 2.99         |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3343  | 0       | 1197   | 12.19  | 2.85     | 3.04         |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5506  | 0       | 896    | 235.72 | 0        | 0            |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5460  | 0       | 1252   | 21.46  | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1301  | 0       | 685    | 8.90   | 4        | 2.32         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3297  | 0       | 750    | 10.65  | 4        | 4            |

_Table 1. Detailed scores of GPT-4o performance across various code-related tasks._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Total Output | Total Time (sec) | Token per Second | Experiment cost ($) | Average Accuracy | Average Completeness | 
|-------------|---------------|--------------|------------------|------------------|---------------------|------------------|----------------------|
| 75686       | 0             | 30858        | 699.277          | 44.128           | 0.50                | 3.439            | 3.200                |

_Table 2. Summary of GPT-4o performance in EPAM's LLMs Benchmark._

This summary shows that GPT-4o has an average accuracy of 3.439 and an average completeness of 3.200 across various code-related tasks. The model processed a total of 75,686 input tokens, generating 30,858 output tokens in 699.277 seconds,
resulting in an average speed of 44.128 tokens per second.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 3.676            | 3.311                | 35.101 seconds | 2206.429       | 62.859                |
| Code Generation    | 3.417            | 3.156                | 17.411 seconds | 1042.714       | 59.889                |
| Code Documentation | 3.271            | 3.149                | 36.855 seconds | 901.556        | 24.462                |

_Table 3. GPT-4o detailed performance analysis across different categories of code-related tasks._

- Code Translation: The model has an average accuracy of 3.676 and an average completeness of 3.311 in translating code between different technologies. It takes an average of 35.101 seconds to process tasks in this category, with an average
  of 2206.429 tokens processed and a speed of 62.859 tokens per second.
- Code Generation: In generating code, GPT-4o has an average accuracy of 3.417 and an average completeness of 3.156. It processes tasks in this category in an average of 17.411 seconds, handling 1042.714 tokens on average at a speed of
  59.889 tokens per second.
- Code Documentation: For generating documentation, the model has an average accuracy of 3.271 and an average completeness of 3.149. It takes an average of 36.855 seconds to process tasks in this category, with an average of 901.556 tokens
  processed and a speed of 24.462 tokens per second.

Large Context Instruction Following (LCIF) score: 87.5%

| Experiment       | Accuracy | Completeness | Time       | Input Tokens | Output Tokens |
|------------------|----------|--------------|------------|--------------|---------------|
| TranslateToReact | 4        | 3            | 52 seconds | 60941        | 2951          |
| UpdateReact      | 4        | 3            | 80 seconds | 67411        | 3481          |

_Table 4. LCIF performance of GPT-4o in EPAM's LLMs Benchmark._

1. TranslateToReact: The model achieved an accuracy of 4 and a completeness of 3 in translating code to React. The task took 52 seconds, processing 60,941 input tokens and generating 2,951 output tokens.
2. UpdateReact: In updating React code, the model achieved an accuracy of 4 and a completeness of 3. The task took 80 seconds, processing 67,411 input tokens and generating 3,481 output tokens.

## Conclusion

GPT-4o 0806 demonstrates strong performance across various code-related tasks, showcasing its capabilities in code translation, generation, and documentation. Here are the key takeaways from the evaluation:

1. Overall Performance:
    - The model achieves solid average scores in both accuracy (3.439) and completeness (3.200) across all experiments, indicating reliable code manipulation capabilities.
    - With an average processing speed of 44.128 tokens per second, GPT-4o 0806 shows moderate efficiency in handling complex programming challenges.

2. Code Translation:
    - GPT-4o 0806 excels in code translation tasks, with the highest average accuracy (3.676) and completeness (3.311) among all categories.
    - The model effectively handles translations between various frameworks (React, Angular, jQuery, Vanilla JS), showcasing its versatility.
    - Translation tasks are processed efficiently, with the highest tokens/second rate (62.859) among all categories.

3. Code Generation:
    - In code generation tasks, the model performs well with an average accuracy of 3.417 and completeness of 3.156.
    - GPT-4o 0806 shows particular strength in generating React components and modifying existing applications.
    - Generation tasks are processed most quickly, with an average time of 17.411 seconds, demonstrating the model's efficiency in this category.

4. Code Documentation:
    - Documentation tasks see solid performance, with average accuracy of 3.271 and completeness of 3.149.
    - The model excels in describing technical implementations and business functionality across different frameworks.
    - However, documentation tasks are processed slower than other categories, at 24.462 tokens per second.

5. Large Context Instruction Following (LCIF):
    - GPT-4o 0806 achieves a strong LCIF score of 87.5%, demonstrating good capability in handling large, complex code bases.
    - The model maintains high accuracy (4/4) and good completeness (3/4) even with substantial input (over 60,000 tokens), showcasing its ability to manage extensive contexts.

6. Efficiency and Cost-Effectiveness:
    - With an experiment cost of $0.50, GPT-4o 0806 offers a good balance between performance and cost.
    - The model's ability to process large amounts of code with good accuracy makes it suitable for a wide range of development tasks.

7. Areas for Improvement:
    - While generally strong, there's room for improvement in certain specific tasks, such as evaluating code quality for Angular (one instance showed 0 accuracy and completeness).
    - Consistency in performance across all task types could be enhanced, as there's some variation in accuracy and completeness scores.

8. Comparison to Previous Version:
    - It's worth noting that the previous model, GPT-4o 0513, showed better results in some areas. This suggests that while GPT-4o 0806 is a strong performer, there might be trade-offs or specific optimizations in the newer version that
      affect certain tasks differently.

In conclusion, GPT-4o 0806 proves to be a capable model for a wide range of code-related tasks. Its consistent performance across translation, generation, and documentation, coupled with strong LCIF capabilities, makes it a versatile tool
for developers. The model's efficiency in processing complex programming tasks, particularly in code translation and generation, is noteworthy. While there are minor areas for potential improvement, the overall performance indicates that
GPT-4o 0806 is a robust and reliable option for advanced code manipulation and analysis tasks.

However, the observation that the previous version (GPT-4o 0513) showed better results in some areas warrants further investigation. It may be beneficial to conduct a more detailed comparison between the two versions to understand the
specific improvements and potential trade-offs in the newer model. This could provide valuable insights for users in choosing the most appropriate version for their specific use cases and for the development team in refining future
iterations of the model.

<p style="text-align: center;">
    © 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>
