# GPT-4o 1120 (2024-11-21)

## Introduction

This article presents an analysis of various code-related tasks performed by the GPT-4o (1120) language model. We'll examine the model's performance across different categories, like translation, generation, and documentation.

## Evaluation Summary

The following table provides detailed information on each experiment conducted. It includes the type of experiment, category, programming language, dataset used, complexity, input and output sizes, time taken, and accuracy and completeness

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Reasons | Output | Time  | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|---------|--------|-------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3269  | 0       | 2585   | 19.00 | 4        | 3            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2343  | 0       | 2208   | 16.33 | 4        | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5413  | 0       | 1638   | 12.59 | 4        | 4            |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3258  | 0       | 2138   | 18.35 | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2275  | 0       | 2017   | 16.16 | 3.27     | 2.82         |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1273  | 0       | 1574   | 11.72 | 2.08     | 3            |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1402  | 0       | 1724   | 12.48 | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 379   | 0       | 384    | 3.19  | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 199   | 0       | 1191   | 9.20  | 4        | 4            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 188   | 0       | 894    | 6.61  | 3.92     | 3            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1230  | 0       | 942    | 7.72  | 2.98     | 2.27         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3226  | 0       | 1604   | 12.36 | 4        | 2            |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5389  | 0       | 1409   | 10.84 | 1.92     | 1.02         |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 15764 | 0       | 1580   | 13.24 | 3        | 2.03         |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1223  | 0       | 1324   | 9.85  | 4        | 3.99         |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3219  | 0       | 1169   | 8.84  | 4        | 4            |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5382  | 0       | 1436   | 11.31 | 3        | 3.2          |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1347  | 0       | 1679   | 12.51 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3343  | 0       | 1956   | 20.59 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5506  | 0       | 1849   | 16.08 | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5460  | 0       | 1674   | 12.77 | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1301  | 0       | 1086   | 7.93  | 2.97     | 3.09         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3297  | 0       | 1648   | 12.35 | 4        | 4            |

_Table 1: Detailed scores of the GPT-4o (1120) model across different experiments._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Total Output | Total Time (sec) | Token per Second | Experiment cost ($) | Average Accuracy | Average Completeness | 
|-------------|---------------|--------------|------------------|------------------|---------------------|------------------|----------------------|
| 75686       | 0             | 35709        | 282.03           | 126.61           | 0.55                | 3.61             | 3.37                 |

_Table 2: Overall performance summary of the GPT-4o (1120) model._

This summary shows that GPT-4o has an average accuracy of 3.61 and an average completeness of 3.37 across various code-related tasks. The model processed a total of 75,686 input tokens, generating 35,709 output tokens in 282.03 seconds,
resulting in an average speed of 126.61 tokens per second.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 3.621            | 3.546                | 15.233 seconds | 1983.429       | 130.206               |
| Code Generation    | 3.403            | 2.617                | 9.023 seconds  | 1143.429       | 126.723               |
| Code Documentation | 3.774            | 3.809                | 12.471 seconds | 1535.667       | 123.142               |

_Table 3: Average performance metrics of GPT-4o (1120) across different experiment categories._

- Code Translation: The model performed well in translating code snippets across different frameworks, with an average accuracy of 3.621 and an average completeness of 3.546. The average time taken for translation was 15.233 seconds, with
  an average speed of 130.206 tokens per second.
- Code Generation: GPT-4o showed good performance in generating code snippets, with an average accuracy of 3.403 and an average completeness of 2.617. The average time taken for code generation was 9.023 seconds, with an average speed of
  126.723 tokens per second.
- Code Documentation: The model excelled in providing code documentation, with an average accuracy of 3.774 and an average completeness of 3.809. The average time taken for documentation was 12.471 seconds, with an average speed of 123.142
  tokens per second.

Large Context Instruction Following (LCIF) score: 87.5%

| Experiment       | Accuracy | Completeness | Time       | Input Tokens | Output Tokens |
|------------------|----------|--------------|------------|--------------|---------------|
| TranslateToReact | 3        | 4            | 40 seconds | 57629        | 3569          |
| UpdateReact      | 3        | 4            | 37 seconds | 60873        | 3443          |

_Table 4: GPT-4o (1120) performance on large context instruction following tasks._

1. TranslateToReact: The model achieved an accuracy score of 3 and a completeness score of 4 in translating a large context instruction to React. The task was completed in 40 seconds, processing 57,629 input tokens and generating 3,569
   output tokens. As in previous versions of GPT-4o, the app didn't launch at the first time due to the loss of the hooks file, but the quality of the code was excellent. It uses hooks, reducers, and all functionalities were successfully
   implemented.
2. UpdateReact: The model achieved an accuracy score of 3 and a completeness score of 4 in updating a React application. The task was completed in 37 seconds, processing 60,873 input tokens and generating 3,443 output tokens. The
   functionality of filtering todos was incorrectly implemented.

## Conclusion

The comprehensive evaluation of GPT-4o (1120) reveals several significant strengths and areas for consideration across different code-related tasks:

### Key Strengths

1. Processing Efficiency
    - Exceptional token processing speed, averaging 126.61 tokens per second across all tasks
    - Consistently high performance with minimal variance: 130.2 tokens/sec for translation, 126.7 tokens/sec for generation, and 123.1 tokens/sec for documentation
    - Efficient handling of large inputs, processing 75,686 total input tokens in just 282.03 seconds

2. Code Documentation Excellence
    - Highest performance in documentation tasks with 3.774 average accuracy and 3.809 completeness
    - Particularly strong in evaluating code quality, achieving perfect scores (4.0) in multiple instances
    - Consistent performance across different code bases and complexity levels

3. Code Translation Capabilities
    - Strong translation performance with 3.621 average accuracy and 3.546 completeness
    - Particularly effective in framework translations (React, Angular, jQuery)
    - Maintained high accuracy even with complex, high-volume translations

### Areas for Improvement

1. Code Generation
    - Lower completeness scores in generation tasks (2.617) compared to other categories
    - Inconsistent performance in test generation, particularly for complex codebases
    - Notable variance in accuracy for different types of generation tasks (ranging from 1.92 to 4.0)

2. Large Context Handling
    - LCIF score of 87.5% indicates room for improvement in handling large contexts
    - Some functionality issues in complex updates (e.g., todo filtering implementation)
    - Occasional loss of supporting files (hooks) in large translations

3. Consistency Across Tasks
    - Performance varies with code complexity and size
    - Lower accuracy in certain specialized tasks (e.g., test generation for legacy code)
    - Some challenges with extra-high complexity tasks

### Cost-Performance Balance

- Reasonable operational costs ($0.55 for the entire test suite)
- Excellent token processing efficiency making it cost-effective for large-scale tasks
- Good balance between speed and accuracy across most use cases

### Recommendations for Use

1. Optimal Use Cases:

    - Code documentation tasks, especially for quality evaluation
    - Framework-to-framework translations
    - Quick prototyping and basic code generation
    - Projects requiring fast processing of large codebases

2. Areas Requiring Additional Support:

    - Complex test generation scenarios
    - Extra-high complexity code bases
    - Large-scale application updates
    - Projects requiring perfect LCIF accuracy

3. Best Practices:

    - Break down complex generation tasks into smaller components
    - Validate generated tests against specific test cases
    - Implement additional verification for large context translations
    - Include explicit hook and dependency management in translation tasks

The GPT-4o (1120) model demonstrates significant improvements in processing efficiency while maintaining high accuracy in most tasks. Its standout performance in documentation and basic translation tasks makes it particularly suitable for
these use cases, while some aspects of code generation and large context handling may benefit from additional development or support tools.

<p align="center">
    © 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0
</p>