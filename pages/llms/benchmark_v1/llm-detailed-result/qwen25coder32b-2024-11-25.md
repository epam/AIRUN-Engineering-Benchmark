# Qwen2.5-Coder 32B

## Introduction

This report provides a detailed evaluation of the Qwen2.5-Coder 32B language model's performance on various code-related tasks. The model was tested on tasks involving code translation, generation, and documentation across different
programming languages and datasets. The analysis includes an overview of the experiments conducted, the categories evaluated, and the model's accuracy and completeness scores.

## Summary

The following table summarizes the overall performance of the Qwen2.5-Coder 32B model across all experiments:

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Reasons | Output | Time  | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|---------|--------|-------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3178  | 0       | 2768   | 29.03 | 4        | 3            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2335  | 0       | 2385   | 25.67 | 4        | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5351  | 0       | 4096   | 42.76 | 4        | 4            |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3169  | 0       | 2814   | 29.84 | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2262  | 0       | 2631   | 27.21 | 2.91     | 4            |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1230  | 0       | 1220   | 13.72 | 2.18     | 3            |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1351  | 0       | 1866   | 19.81 | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 374   | 0       | 368    | 4.20  | 4        | 2.92         |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 200   | 0       | 820    | 8.74  | 3.05     | 3.08         |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 189   | 0       | 214    | 2.55  | 3        | 4            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1186  | 0       | 1191   | 13.33 | 2.97     | 3.06         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3134  | 0       | 2491   | 25.98 | 4        | 2.97         |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5324  | 0       | 2399   | 25.32 | 2.75     | 0.92         |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 15429 | 0       | 4096   | 43.70 | 4        | 2.98         |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1180  | 0       | 653    | 7.34  | 4        | 3.92         |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3128  | 0       | 836    | 9.05  | 3.01     | 3.01         |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5318  | 0       | 842    | 9.28  | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1304  | 0       | 1167   | 12.34 | 3.32     | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3252  | 0       | 1289   | 13.20 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5442  | 0       | 1458   | 16.23 | 1.85     | 4            |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5395  | 0       | 1138   | 13.36 | 4        | 3.38         |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1257  | 0       | 747    | 8.56  | 3        | 1.99         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3205  | 0       | 1418   | 14.82 | 4        | 4            |

_Table 1. Detailed evaluation of Qwen2.5-Coder 32B model in EPAM's LLMs benchmark._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Total Output | Total Time (sec) | Token per Second | Experiment cost ($) | Average Accuracy | Average Completeness |
|-------------|---------------|--------------|------------------|------------------|---------------------|------------------|----------------------|
| 74193       | 0             | 38907        | 416.04           | 93.52            | 0.10                | 3.48             | 3.40                 |

_Table 2. Summary of Qwen2.5-Coder 32B model performance across all experiments._

This summary shows that Qwen2.5-Coder 32B achieved an average accuracy score of 3.48 and an average completeness score of 3.40 across all experiments. The model processed a total of 74,193 tokens with an average speed of 93.52 tokens per
second.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 3.584            | 3.714                | 26.864 seconds | 2540.000       | 94.552                |
| Code Generation    | 3.396            | 2.847                | 17.689 seconds | 1654.143       | 93.511                |
| Code Documentation | 3.464            | 3.589                | 11.575 seconds | 1060.889       | 91.657                |

_Table 3. Detailed analysis of Qwen2.5-Coder 32B model performance across different experiment categories._

- Code Translation: The model performed well in code translation tasks, achieving an average accuracy score of 3.584 and an average completeness score of 3.714. The average processing time for code translation tasks was 26.864 seconds, with
  an average speed of 94.552 tokens per second.
- Code Generation: The model showed good performance in code generation tasks, with an average accuracy score of 3.396 and an average completeness score of 2.847. The average processing time for code generation tasks was 17.689 seconds,
  with an average speed of 93.511 tokens per second.
- Code Documentation: The model demonstrated strong performance in code documentation tasks, achieving an average accuracy score of 3.464 and an average completeness score of 3.589. The average processing time for code documentation tasks
  was 11.575 seconds, with an average speed of 91.657 tokens per second.

Large Context Instruction Following (LCIF) score: 71.9%

| Experiment       | Accuracy | Completeness | Time       | Input Tokens | Output Tokens |
|------------------|----------|--------------|------------|--------------|---------------|
| TranslateToReact | 3        | 2.5          | 71 seconds | 101422       | 5507          |
| UpdateReact      | 3        | 3            | 54 seconds | 66164        | 4413          |

_Table 4. Large Context Instruction Following (LCIF) score for Qwen2.5-Coder 32B model._

1. TranslateToReact: The model achieved an accuracy score of 3 and a completeness score of 2.5 in the TranslateToReact task. The processing time was 71 seconds, with 101,422 input tokens and 5,507 output tokens.
2. UpdateReact: The model performed well in the UpdateReact task, scoring 3 for accuracy and 3 for completeness. The processing time was 54 seconds, with 66,164 input tokens and 4,413 output tokens.

## Conclusion

The comprehensive evaluation of Qwen2.5-Coder 32B reveals a model with strong capabilities in code-related tasks, particularly excelling in code translation while maintaining consistent performance across various programming languages and
complexity levels. Notably, as an open-source model, it demonstrates remarkable competitiveness against proprietary solutions, outperforming models like GPT-4o (2024-08-06) and Gemini 1.5 Pro in several key metrics.

### Strengths

1. **Code Translation Excellence**
    - Achieved the highest accuracy (3.584) and completeness (3.714) scores in translation tasks across all categories
    - Demonstrated particular proficiency in framework migrations (jQuery → React, Angular → React)
    - Maintained consistent performance across varying complexity levels
    - Processed code efficiently at 94.552 tokens/second

2. **Documentation Capabilities**
    - Strong performance in documentation tasks with 3.464 accuracy and 3.589 completeness
    - Excelled in business functionality documentation
    - Demonstrated quick processing times (average 11.575 seconds)
    - Showed adaptability across different codebase sizes

3. **Processing Efficiency**
    - Maintained consistent processing speed of 93.52 tokens/second overall
    - Demonstrated cost-effective operation ($0.10 total experiment cost)
    - Handled 74,193 input tokens and generated 38,907 output tokens
    - Completed all tasks within reasonable timeframes (416.04 seconds total)

### Areas for Improvement

1. **Large Context Handling**
    - LCIF score of 71.9% indicates room for improvement
    - Variable performance in completeness scores for large-scale translations

2. **Code Generation**
    - Lower completeness scores (2.847) in generation tasks
    - Struggled with complex test generation for large codebases
    - Inconsistent performance in generating tests for legacy code

3. **Quality Assessment**
    - Variable performance in code quality evaluation tasks
    - Inconsistent accuracy scores for technical implementation documentation
    - Lower performance in analyzing extra-high complexity codebases

### Open-Source Advantage

1. **Market Position**
    - Outperforms Gemini 1.5 Pro and matches GPT-4o (2024-08-06) in overall metrics
    - Demonstrates superior code translation capabilities (90.2%) compared to many commercial alternatives

2. **Accessibility Benefits**
    - Allows for local deployment and customization
    - Enables organizations to maintain data privacy and security
    - Provides cost-effective alternative to subscription-based services
    - Supports community-driven improvements and adaptations

3. **Enterprise Viability**
    - Offers competitive performance at a fraction of the cost of proprietary solutions
    - Enables integration into existing development workflows without vendor lock-in
    - Provides flexibility for custom fine-tuning and optimization
    - Demonstrates enterprise-grade capabilities in specific tasks

### Recommendations for Use

1. **Optimal Use Cases**
    - Framework migration projects, especially for modern web frameworks
    - Documentation generation for small to medium-sized codebases
    - Quick code analysis and translation tasks
    - Projects requiring cost-effective processing
    - Organizations prioritizing data privacy and local deployment

2. **Considerations**
    - Implement additional validation for generated tests
    - Consider supplementary tools for large-context operations
    - Plan for longer processing times with extensive codebases
    - Validate output quality for critical code generation tasks

The Qwen2.5-Coder 32B model represents a significant achievement in open-source AI, demonstrating that freely available models can compete effectively with proprietary solutions. Its strong performance across key metrics, particularly in
code translation and documentation, combined with the advantages of open-source deployment, makes it an attractive option for organizations seeking to integrate AI into their development workflows. While there are areas for improvement,
particularly in handling larger contexts and generating complex tests, the model's combination of performance, accessibility, and cost-effectiveness positions it as a viable alternative to commercial solutions.

<p align="center">
    © 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0
</p>