# GPT-4o mini 0718 (2024-11-06)

## Introduction

This article presents an analysis of various code-related tasks performed by the GPT-4o mini language model. We'll examine the model's performance across different categories, like translation, generation, and documentation.

## Evaluation Summary

The following table provides detailed information on each experiment conducted. It includes the type of experiment, category, programming language, dataset used, complexity, input and output sizes, time taken, and accuracy and completeness

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Reasons | Output | Time  | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|---------|--------|-------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3269  | 0       | 2324   | 65.49 | 3.85     | 4            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2343  | 0       | 1818   | 42.39 | 4        | 3.01         |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5413  | 0       | 922    | 11.51 | 4        | 0.96         |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3258  | 0       | 1589   | 26.35 | 4        | 1.88         |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2275  | 0       | 1861   | 39.45 | 1.94     | 1.73         |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1273  | 0       | 1274   | 16.37 | 2.08     | 4            |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1402  | 0       | 1732   | 30.20 | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 379   | 0       | 367    | 5.88  | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 199   | 0       | 855    | 13.31 | 3        | 3            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 188   | 0       | 609    | 9.11  | 3.32     | 4            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1230  | 0       | 899    | 14.19 | 2.91     | 1.94         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3226  | 0       | 1457   | 18.84 | 1.95     | 1.04         |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5389  | 0       | 1104   | 21.57 | 1.88     | 0.9          |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 15764 | 0       | 877    | 15.14 | 3        | 3.08         |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1223  | 0       | 709    | 10.34 | 1.99     | 4            |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3219  | 0       | 817    | 18.67 | 3.01     | 3.02         |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5382  | 0       | 897    | 17.11 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1347  | 0       | 1096   | 22.06 | 2.82     | 2.62         |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3343  | 0       | 1005   | 18.04 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5506  | 0       | 1243   | 16.39 | 2.95     | 4            |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5460  | 0       | 852    | 19.36 | 4        | 1.97         |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1301  | 0       | 678    | 12.69 | 4        | 2.18         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3297  | 0       | 1002   | 14.27 | 4        | 4            |

_Table 1. Summary of GPT-4o mini 0718 performance across various code-related tasks._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Total Output | Total Time (sec) | Token per Second | Experiment cost ($) | Average Accuracy | Average Completeness |
|-------------|---------------|--------------|------------------|------------------|---------------------|------------------|----------------------|
| 75686       | 0             | 25987        | 478.74           | 54.28            | 0.03                | 3.25             | 2.93                 |

_Table 2. GPT-4o mini 0718 performance summary in EPAM's LLMs Benchmark._

This summary shows that GPT-4o mini 0718 has an average accuracy of 3.25 and an average completeness of 2.93 across various code-related tasks. The model processed a total of 75,686 input tokens, generating 25,987 output tokens in 478.74
seconds, resulting in an average speed of 54.28 tokens per second.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 3.410            | 2.797                | 33.108 seconds | 1645.714       | 49.707                |
| Code Generation    | 2.866            | 2.566                | 14.007 seconds | 881.143        | 62.906                |
| Code Documentation | 3.419            | 3.310                | 16.548 seconds | 922.111        | 55.724                |

_Table 3. GPT-4o mini 0718 detailed performance analysis across different categories._

- Code Translation: The model performs moderately well in translating code between different technologies, with an average accuracy of 3.41 and an average completeness of 2.80. The average time taken for translation tasks is 33.11 seconds,
  with an average of 1645.71 tokens processed and a speed of 49.71 tokens per second.
- Code Generation: GPT-4o mini 0718 shows lower performance in code generation tasks, with an average accuracy of 2.87 and an average completeness of 2.57. The model processes these tasks at an average speed of 62.91 tokens per second.
- Code Documentation: The model excels in generating documentation for code, with an average accuracy of 3.42 and an average completeness of 3.31. The average time taken for documentation tasks is 16.55 seconds, with an average of 922.11
  tokens processed and a speed of 55.72 tokens per second.

Large Context Instruction Following (LCIF) score: 56.25%

| Experiment       | Accuracy | Completeness | Time       | Input Tokens | Output Tokens |
|------------------|----------|--------------|------------|--------------|---------------|
| TranslateToReact | 3        | 2            | 54 seconds | 52539        | 1954          |      
| UpdateReact      | 2        | 2            | 53 seconds | 54310        | 3123          |         

_Table 4. GPT-4o mini 0718 detailed performance analysis for specific experiments._

1. TranslateToReact: The model achieved an accuracy of 3 and a completeness of 2 in translating code to React. The task took 54 seconds, processing 52,539 input tokens and generating 1,954 output tokens.
2. UpdateReact: In updating React code, the model achieved an accuracy of 2 and a completeness of 2. The task took 53 seconds, processing 54,310 input tokens and generating 3,123 output tokens.

## Conclusion

GPT-4o mini 0718 demonstrates moderate performance across various code-related tasks, with strengths in certain areas and room for improvement in others. Here are the key takeaways from the evaluation:

1. Overall Performance:
    - The model achieves an average accuracy of 3.25 and completeness of 2.93 across all experiments, indicating a reasonable but not exceptional performance in code manipulation tasks.
    - With an average processing speed of 54.28 tokens per second, GPT-4o mini 0718 shows moderate efficiency in handling complex programming challenges.

2. Code Translation:
    - The model performs moderately well in code translation tasks, with an average accuracy of 3.41 and completeness of 2.80.
    - Performance varies significantly across different translation tasks, with some achieving high scores (e.g., ReactToAngular for ToDoApp_ReactJS) and others struggling (e.g., UpdateAngular for ToDoApp_AngularJS).
    - Translation tasks take longer on average (33.11 seconds) compared to other categories, possibly due to the complexity of cross-framework conversions.

3. Code Generation:
    - GPT-4o mini 0718 shows lower performance in code generation tasks, with average accuracy of 2.87 and completeness of 2.57.
    - The model struggles particularly with writing tests for legacy code, indicating a potential area for improvement.
    - Despite lower accuracy, generation tasks are processed most efficiently at 62.91 tokens per second.

4. Code Documentation:
    - Documentation tasks see the best performance, with the highest average accuracy (3.42) and completeness (3.31) among all categories.
    - The model excels in describing business functionality and evaluating code quality for certain frameworks.
    - Documentation tasks are processed efficiently, with an average time of 16.55 seconds per task.

5. Large Context Instruction Following (LCIF):
    - GPT-4o mini 0718 achieves a relatively low LCIF score of 56.25%, indicating challenges in handling large, complex code bases.
    - Performance in TranslateToReact and UpdateReact tasks was moderate, with accuracy and completeness scores of 2-3 out of 4.
    - The model processes large inputs (over 50,000 tokens) relatively quickly but struggles with maintaining high accuracy and completeness.

6. Efficiency and Cost-Effectiveness:
    - With a very low experiment cost of $0.03, GPT-4o mini 0718 is highly cost-effective, making it suitable for scenarios where budget constraints are a primary concern.
    - The model's efficiency varies across tasks, with generation tasks being the most efficient and translation tasks the least.

7. Areas for Improvement:
    - Code generation, particularly writing tests for legacy code, is a clear area for improvement.
    - Consistency in performance across different frameworks and task types could be enhanced.
    - The model's ability to handle large contexts (LCIF) needs significant improvement to be competitive with more advanced models.

In conclusion, GPT-4o mini 0718 demonstrates a balance between cost-effectiveness and moderate performance in code-related tasks. It excels in code documentation and shows promise in certain translation tasks, making it a potentially useful
tool for specific development scenarios, especially where budget constraints are a primary concern. However, its limitations in code generation and handling large contexts suggest that it may not be suitable for all complex programming
tasks. The model could be particularly valuable in scenarios where quick, cost-effective code documentation or simple code translations are needed, but may require human oversight or complementary tools for more complex tasks. Future
iterations could focus on improving code generation capabilities and enhancing the model's ability to handle larger, more complex codebases to increase its versatility and applicability in more demanding software development environments.

<p style="text-align: center;">
    Â© 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>