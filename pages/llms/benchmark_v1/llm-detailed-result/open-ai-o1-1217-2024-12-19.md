# OpenAI o1 (2024-12-17)

## Introduction

This report provides a detailed evaluation of OpenAI's o1 (1217) language model's performance on various code-related tasks. The model was tested on tasks involving code translation, generation, and documentation across different
programming languages and datasets. The analysis includes experiments conducted, categories evaluated, and the model's accuracy and completeness scores.

## Evaluation Summary

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Reasons | Output | Total Output | Time   | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|---------|--------|--------------|--------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3268  | 1472    | 4053   | 5525         | 40.18  | 4        | 4            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2342  | 4416    | 3311   | 7727         | 63.61  | 4        | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5412  | 1728    | 5853   | 7581         | 97.48  | 4        | 4            |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3257  | 2240    | 2981   | 5221         | 43.44  | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2274  | 2688    | 4031   | 6719         | 50.05  | 4        | 4            |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1272  | 2432    | 1717   | 4149         | 33.52  | 4        | 4            |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1401  | 2688    | 2895   | 5583         | 149.40 | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 378   | 1408    | 308    | 1716         | 12.69  | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 198   | 1344    | 1249   | 2593         | 30.60  | 4        | 4            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 187   | 1280    | 690    | 1970         | 20.35  | 3.02     | 3            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1229  | 1536    | 1650   | 3186         | 24.67  | 3.08     | 3.82         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3225  | 704     | 4916   | 5620         | 42.40  | 4        | 2.03         |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5388  | 1280    | 1225   | 2505         | 20.35  | 2.5      | 1.94         |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 15763 | 576     | 1505   | 2081         | 33.95  | 3.01     | 2.15         |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1222  | 128     | 710    | 838          | 59.33  | 2.22     | 4            |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3218  | 256     | 1025   | 1281         | 14.32  | 3.5      | 4            |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5381  | 512     | 1410   | 1922         | 15.68  | 2.98     | 2.5          |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1346  | 896     | 2281   | 3177         | 25.88  | 3.62     | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3342  | 704     | 2511   | 3215         | 27.82  | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5505  | 576     | 2464   | 3040         | 29.23  | 3.73     | 2.03         |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5459  | 896     | 2243   | 3139         | 23.59  | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1300  | 384     | 985    | 1369         | 11.71  | 4        | 3.01         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3296  | 1216    | 1798   | 3014         | 37.32  | 4        | 4            |

_Table 1. Detailed evaluation of OpenAI o1 (1217) in EPAM's LLMs Benchmark._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Output (payload) | Total Output | Total Time (sec) | Token per Second | Experiment cost ($) | Average Accuracy | Average Completeness | 
|-------------|---------------|------------------|--------------|------------------|------------------|---------------------|------------------|----------------------|
| 75663       | 31360         | 51811            | 83171        | 907.56           | 91.64            | 6.13                | 3.64             | 3.50                 |

_Table 2. Summary of OpenAI o1 (1217) performance in EPAM's LLMs Benchmark._

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 4.000            | 4.000                | 68.238 seconds | 6072.143       | 88.984                |
| Code Generation    | 3.373            | 2.991                | 26.431 seconds | 2810.143       | 106.321               |
| Code Documentation | 3.561            | 3.504                | 27.209 seconds | 2332.778       | 85.736                |

_Table 3. Detailed analysis of OpenAI o1 (1217) performance in EPAM's LLMs Benchmark._

- Code Translation: The model excelled in code translation tasks, achieving perfect scores with an average accuracy and completeness of 4.0. The average processing time was 68.238 seconds, maintaining a stable speed of 88.984 tokens per
  second.
- Code Generation: Performance in code generation showed good accuracy (3.373) and completeness (2.991), with faster processing times averaging 26.431 seconds and increased token processing speed of 106.321 tokens per second.
- Code Documentation: Documentation tasks demonstrated strong performance with 3.561 accuracy and 3.504 completeness, processing at an efficient rate of 85.736 tokens per second with an average time of 27.209 seconds.

Large Context Instruction Following (LCIF) score: 96.88%

| Experiment       | Accuracy | Completeness | Time | Input Tokens | Reasoning | Output Tokens | Output Total Tokens |
|------------------|----------|--------------|------|--------------|-----------|---------------|---------------------|
| TranslateToReact | 4        | 3.5          | 137  | 71725        | 9600      | 13469         | 23094               |
| UpdateReact      | 4        | 4            | 166  | 70861        | 9920      | 13703         | 23623               |

_Table 4. OpenAI o1 (1217) performance in Large Context Instruction Following (LCIF) tasks._

LCIF experiment notes: OpenAI o1 (1217) demonstrated exceptional performance in following complex instructions and handling large contexts, achieving a near-perfect LCIF score of 96.88%.

## Conclusion

The comprehensive evaluation of OpenAI o1 (1217) reveals a high-performing model with exceptional capabilities across all tested domains. Key findings include:

1. Performance Across Categories:
    - Perfect scores in Code Translation (4.0/4.0)
    - Strong performance in Code Documentation (3.561/3.504)
    - Solid results in Code Generation (3.373/2.991)
    - Outstanding LCIF score (96.88%)

2. Operational Efficiency:
    - Consistent token processing speeds ranging from 85-106 tokens per second
    - Efficient handling of large datasets with predictable processing times
    - Higher processing speeds in documentation and generation tasks compared to translation

3. Strengths:
    - Perfect accuracy and completeness in code translation tasks
    - Exceptional performance in following complex instructions
    - Consistent high scores across different framework combinations
    - Strong performance in both small and large codebase scenarios

4. Areas for Improvement:
    - Higher computational cost ($6.13 for test suite)
    - Slightly lower completeness scores in code generation tasks
    - Longer processing times for translation tasks compared to other categories

Overall, OpenAI o1 (1217) demonstrates exceptional performance across all evaluated categories, positioning it as a top-tier model in the current LLM landscape. Its perfect scores in code translation and near-perfect LCIF performance are
particularly noteworthy, though the higher computational cost may be a consideration for some use cases. The model's consistent high performance and reliability make it an excellent choice for professional development environments where
accuracy and completeness are paramount.

<p style="text-align: center;">
    Â© 2025 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>