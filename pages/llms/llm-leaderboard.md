# LLMs Leaderboard (2024-11-08 Update)

This page compares the effectiveness of Anthropic, Google, xAI, OpenAI, Meta and others companies LLMs in executing software engineering tasks, including code translation, code generation, documentation generation and large context
instruction following (LCIF).

Our latest research and evaluations have revealed significant shifts in large language model performance. Leading our rankings is OpenAI's o1-mini model, confirming OpenAI's claims about its superior coding capabilities compared to
o1-preview. Following closely is Anthropic's Claude 3.5 Sonnet v2, version 2024-10-22, demonstrating impressive results. The third place is secured by OpenAI's ChatGPT-4o, widely used in their chat interfaces.

Notably, GPT-4o (version 2024-11-20) has shown a record-breaking token generation speed, averaging 126 tokens per second. This has significantly impacted the model's overall score, propelling it into the top 5 and substantially improving
its performance compared to previous versions (0513 and 0806). Considering the balance of cost, speed, and performance, GPT-4o now presents an attractive option for practical applications.

Rounding out the top five is Anthropic's Claude 3.5 Haiku - a new model that, despite its compact nature, demonstrates high efficiency at a low cost of use. Haiku's appearance in the top 5 underscores Anthropic's continuous innovations in
AI technology.

The current top of Large Language Models based on research, in order of Total Score, are:

1. OpenAI o1-mini (2024-09-12) - 91.7%
2. Claude 3.5 Sonnet v2 (2024-10-22) - 89.4%
3. ChatGPT-4o (latest) - 89.6%
4. GPT-4o (2024-11-20) - 88.7%
5. Claude 3.5 Haiku (2024-10-22) - 86.5%

You can read [llm-comparison-report.md](llm-comparison-report.md) for a detailed comparison of the models. The report includes all experiment scores, the performance of each model in different categories, and the final scores.

## Final scores in EPAM's LLMs Benchmark

All categories, except Large Context Instructions Following (LCIF), are evaluated automatically using the Large Language Model<sup>1</sup>
You can read detailed reports on concrete model performance and the areas where each model excels
in [llm-detailed-result](llm-detailed-result).

The table below provides an overview of the experiment, encompassing three categories. Here, you can review the average accuracy and completeness across all categories, total execution time,
total number of input and output tokens (for o1-series models, output tokens include reasoning tokens), the cost of the experiment, and finally, the total score.
The total score comprises the average score across all categories including LCIF experiment score.

| Model                                                                                  | Total Time (min) | Total input | Total output | Cost ($) | Total Score |
|----------------------------------------------------------------------------------------|------------------|-------------|--------------|----------|-------------|
| [OpenAI o1-mini (2024-09-12)](llm-detailed-result/open-ai-o1-mini-2024-11-06.md)       | 11.23            | 77314       | 102043       | 1.46     | **91.7%**   |
| [Claude 3.5 Sonnet v2](llm-detailed-result/claude-35-sonnet-2024-11-06.md)             | 11.11            | 94643       | 39652        | 0.88     | 89.6%       |
| [ChatGPT-4o](llm-detailed-result/chat-gpt-4o-2024-11-06.md)                            | 7.34             | 75686       | 37560        | 0.94     | 89.4%       |
| [GPT-4o (2024-11-20)](llm-detailed-result/gpt-4o-1120-2024-11-21.md)                   | 4.7              | 75686       | 35709        | 0.55     | 88.7%       |
| [Claude 3.5 Haiku](llm-detailed-result/claude-35-haiku-2024-11-06.md)                  | 8.93             | 94643       | 35217        | 0.27     | 86.5%       |
| [OpenAI o1-preview (2024-09-12)](llm-detailed-result/open-ai-o1-preview-2024-11-06.md) | 46.73            | 77314       | 150303       | 10.18    | 85.2%       |
| [Qwen 2.5 Coder 32B](llm-detailed-result/qwen25coder32b-2024-11-25.md)                 | 6.93             | 74193       | 38907        | 0.10     | 82.4%       |
| [GPT-4o (2024-08-06)](llm-detailed-result/gpt-4o-0806-2024-11-06.md)                   | 11.65            | 75686       | 30858        | 0.50     | 82.2%       |
| [Gemini 1.5 Pro (002)](llm-detailed-result/gemini-15-pro-2024-11-08.md)                | 16.65            | 87809       | 36669        | 0.29     | 81.0%       |
| [Grok Beta](llm-detailed-result/grok-beta-2024-11-15.md)                               | 10.45            | 75457       | 32444        | 0.86     | 77.8%       |
| Llama3.1 405B<sup>2</sup>                                                              | 9.56             | 74045       | 27498        | 0.30     | 71.2%       |
| [GPT-4o-mini (0718)](llm-detailed-result/gpt-4o-mini-0718-2024-11-06.md)               | 7.98             | 75686       | 25987        | 0.03     | 70.8%       |
| GPT-3.5 Turbo<sup>2</sup>                                                              | 4.48             | 73956       | 17270        | 0.06     | 60.9%       |
| Llama3 70B<sup>2</sup>                                                                 | 30.96            | 58615       | 23784        | 0.05     | 59.7%       |
| Claude 3 Opus<sup>2</sup>                                                              | 24.59            | 94480       | 45574        | 4.84     | -           |
| GPT-4o (2024-05-13)<sup>2</sup>                                                        | 12.68            | 75581       | 42729        | 1.02     | -           |
| Gemini 1.5 Pro (0801-exp)<sup>2</sup>                                                  | 15.66            | 87887       | 44631        | 0.78     | -           |
| Gemini 1.5 Pro (0409)<sup>2</sup>                                                      | 17.40            | 87596       | 36497        | 1.38     | -           |

_Table 1. Overview of the experiment results._

> Additional info:  
> <sup>1</sup> - Evaluation and grading of benchmark results were performed automatically with the help of evaluation tools based on the GPT-4o model.
> Please refer the page [Automated Evaluation with LLMs](automated-evaluation-with-llms.md) to learn about the evaluation tool.  
> <sup>2</sup> - Evaluation was done more than 3 month ago, results are outdated


We have visualized some of the information for your review.

![llm-rating.png](../../images/llms/llm-rating.png)
![llm-price.png](../../images/llms/llm-price.png)
![llm-tokens.png](../../images/llms/llm-tokens.png)
![llm-cost.png](../../images/llms/llm-cost.png)
![llm-time.png](../../images/llms/llm-time.png)

## Models specification

| LLM Name                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Context Window            | Cost (Input / Output per MTok) | Max Output | Training Data   |
|-----------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|--------------------------------|------------|-----------------|
| OpenAI o1-mini                                                        | The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user. o1-mini: faster and cheaper reasoning model particularly good at coding, math, and science. <br>API Provider: OpenAI, Azure                                                                                                                                                                                                                                           | 128K                      | $3.00 / $12.00                 | 64K        | Up to Oct 2023  |
| Claude 3.5 Sonnet v2                                                  | Anthropic's most powerful AI model. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet. Claude 3.5 Sonnet shows us the frontier of what's possible with generative AI.<br>Parameters: Unknown<br>API Provider: AWS Bedrock, Google Vertex AI                                                                                                                                                                                           | 200K                      | $3.00 / $15.00                 | 8K         | April, 2024     |
| ChatGPT-4o (0903)                                                     | Dynamic model continuously updated to the current version of GPT-4o in ChatGPT. Intended for research and evaluation*. *We are releasing this model for developers and researchers to explore OpenAI's latest research. For production use, OpenAI recommends using dated GPT models, which are optimized for API usage.<br>Parameters: Unknown<br>API Provider: OpenAI, Azure                                                                                                                                                                                                                   | 128K                      | $5.00 / $15.00                 | 16K        | Up to Oct 2023  |
| GPT-4o (2024-11-20)                                                   | [Update to GPT-4o (November 20, 2024)](https://help.openai.com/en/articles/9624314-model-release-notes): We’ve updated GPT-4o for ChatGPT users on all paid tiers. This update to GPT-4o includes improved writing capabilities that are now more natural, audience-aware, and tailored to improve relevance and readability. This model is also better at working with uploaded files, providing deeper insights and more thorough responses.<br>Parameters: Unknown<br>API Provider: OpenAI, Azure                                                                                             | 128K                      | $2.50 / $10.00                 | 16K        | Up to Oct 2023  |
| Claude 3.5 Haiku                                                      | Claude 3.5 Haiku matches the performance of Claude 3 Opus, our prior largest model, on many evaluations at a similar speed to the previous generation of Haiku.<br>Parameters: Unknown<br>API Provider: AWS Bedrock, Google Vertex AI                                                                                                                                                                                                                                                                                                                                                            | 200K                      | $1.00 / $5.00                  | 8K         | July, 2024      |
| OpenAI o1-preview                                                     | The o1 series of large language models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user. o1-preview: reasoning model designed to solve hard problems across domains.<br>API Provider: OpenAI, Azure                                                                                                                                                                                                                                                            | 128K                      | $15.00 / $60.00                | 32K        | Up to Oct 2023  |
| [Qwen2.5-Coder 32B Instruct](https://github.com/QwenLM/Qwen2.5-Coder) | Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers.</br>Parameters: 32B</br>API Provider: [Fireworks.ai](https://fireworks.ai/models/fireworks/qwen2p5-coder-32b-instruct), other [available API](https://artificialanalysis.ai/models/qwen2-5-coder-32b-instruct/providers)                                                                                                     | 33K (depends on provider) | $0.90 / $0.90                  | 4K         | -               |
| GPT-4o (2024-08-06)                                                   | Updated snapshot of GPT-4o model with an increased output token size and a reduced price<br>Parameters: Unknown<br>API Provider: OpenAI, Azure                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 128K                      | $2.50 / $10.00                 | 16K        | Up to Oct 2023  |
| Gemini 1.5 Pro (002)                                                  | A multimodal model that supports adding image, audio, video, and PDF files in text or chat prompts for a text or code response. This model supports long-context understanding up to the maximum input token limit.<br>Parameters: Unknown<br>API Provider: Google AI Studio                                                                                                                                                                                                                                                                                                                     | 2M                        | $1.25 / $5.00                  | 8K         | September, 2024 |
| [Grok Beta](https://x.ai/blog/api)                                    | [Grok Beta](https://docs.x.ai/docs#models) for now is only one model available via xAI API. Comparable performance to Grok 2 but with improved efficiency, speed and capabilities.<br/>xAI blog: "We are kicking off our public beta with a preview of a new Grok model that is currently in the final stages of development. The model – released under the provisional name grok-beta – has a context length of 128,000 tokens and also supports function calling and system prompts. We will release more information about the model shortly."<br/>Parameters: Unknown<br/>API Provider: xAI | 128K                      | $5.00 / $15.00                 | 4K         | -               |
| GPT-4o mini                                                           | GPT-4o mini ("o" for "omni") is our most advanced model in the small models category, and our cheapest model yet. It is multimodal (accepting text or image inputs and outputting text), has higher intelligence than gpt-3.5-turbo but is just as fast. It is meant to be used for smaller tasks, including vision tasks. We recommend choosing gpt-4o-mini where you would have previously used gpt-3.5-turbo as this model is more capable and cheaper.<br>Parameters: Over 70B<br>API Provider: OpenAI, Azure                                                                                | 128K                      | $0.15 / $0.60                  | 16K        | Up to Oct 2023  |
| Llama3.1 405B                                                         | The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. <br>Parameters: 405B <br>API Provider: Fireworks.ai                                                                                                                     | 128K                      | $3.00 / $3.00                  | 16K        | Up to Dec 2023  |
| GPT-3.5 Turbo                                                         | GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, gpt-4o-mini should be used in place of gpt-3.5-turbo, as it is cheaper, more capable, multimodal, and just as fast. gpt-3.5-turbo is still available for use in the API.<br>Parameters: Unknown <br>API Providers: OpenAI, Azure                                                                                                                                                            | 16K                       | $0.50 / $1.50                  | 4K         | Up to Dec 2023  |
| Llama3 70B                                                            | Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pre-trained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. <br> Parameters: 70B <br>API Provider: EPAM AI DIAL<sup>*</sup>                                                                              | 8K                        | $0.59 / $0.79                  | 4K         | Up to Dec 2023  |
| Claude 3 Opus                                                         | Claude 3 Opus is Anthropic's most powerful AI model, with state-of-the-art performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Claude 3 Opus shows us the frontier of what's possible with generative AI.<br>Parameters: 2T (not official number) - https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3<br>API Provider: AWS Bedrock, Google Vertex AI                                                                                                                   | 200K                      | $15.00 / $75.00                | 4K         | August, 2023    |
| Gemini 1.5 Pro (Experimental 0801)                                    | Experimental preview version of Google's best performing multimodal model with features for a wide variety of reasoning tasks<br>Parameters: Unknown<br>API Provider: Google AI Studio                                                                                                                                                                                                                                                                                                                                                                                                           | 2M                        | $3.50 / $10.50                 | 8K         | November, 2023  |
| GPT-4o (2024-05-13)                                                   | GPT-4o ("o" for "omni") is our most advanced model. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper. Additionally, GPT-4o has the best vision and performance across non-English languages of any of our models. GPT-4o is available in the OpenAI API to paying customers<br>Parameters: Over 200B - Medium article<br>API Provider: OpenAI, Azure                                                                                        | 128K                      | $5.00 / $15.00                 | 4K         | Up to Oct 2023  |
| Gemini 1.5 Pro (0409)                                                 | Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.<br>Parameters: Unknown<br>API Provider: Vertex AI                                                                                                                                                                                                                                                                            | 1M                        | $7.00 / $21.00                 | 8K         | November, 2023  |
| GPT-4 Turbo (2024-04-09)                                              | The latest generation of the multimodal model from OpenAI. It has updated knowledge Up to Dec 2023 and provides 3X cheaper pricing for input tokens and 2X cheaper for output tokens compared to the original GPT-4 model.<br>Parameters: Unknown<br>API Provider: OpenAI, Azure                                                                                                                                                                                                                                                                                                                 | 128K                      | $10.00 / $30.00                | 4K         | Up to Dec 2023  |

_Table 2. Description of LLMs, their versions, amount of information to process, and the length of the response._

> <sup>*</sup>EPAM AI DIAL is a proprietary tool developed by EPAM Systems, serves as a platform for hosting and utilizing models while also functioning as a proxy through Azure.

## Notes

- To learn more about our methodology for evaluating LLMs, please read [LLMs Benchmark Approach](llm-approach.md).
- To submit your model for evaluation, please refer to the [Large Language Model Benchmark Submission](submissions/llm-benchmark-submission.md) page.

<p align="center">
    © 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0
</p>