# Gemini Experimental (2024-12-06)

## Introduction

This report provides a detailed evaluation of the Gemini Experimental language model's performance on various code-related tasks. The model was tested on tasks involving code translation, generation, and documentation across different
programming languages and datasets. The analysis includes an overview of the experiments conducted, the categories evaluated, and the model's accuracy and completeness scores.

## Summary

The following table summarizes the performance of the Gemini Experimental model across all experiments:

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Output | Time   | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|--------|--------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3817  | 3201   | 75.51  | 3.03     | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 6338  | 8024   | 186.74 | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2591  | 4574   | 106.21 | 4        | 4            |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1598  | 3328   | 78.94  | 4        | 4            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2766  | 3705   | 86.74  | 4        | 4            |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1457  | 1749   | 41.03  | 4        | 3.02         |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3807  | 3946   | 91.65  | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 416   | 421    | 10.93  | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 201   | 1337   | 31.82  | 4        | 4            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 192   | 830    | 20.28  | 4        | 4            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1409  | 1983   | 47.01  | 2.08     | 3.22         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3769  | 1278   | 30.34  | 2.99     | 0.5          |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 6309  | 4247   | 97.33  | 1.95     | 0.98         |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 18138 | 8192   | 193.46 | 4        | 4            |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1395  | 1114   | 29.17  | 4        | 4            |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3755  | 1034   | 25.01  | 3.12     | 2.99         |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 6295  | 1210   | 31.68  | 2        | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1532  | 3023   | 70.14  | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3892  | 4820   | 111.83 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 6432  | 6735   | 155.87 | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 6380  | 1256   | 190.05 | 4        | 2.01         |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1480  | 442    | 189.75 | 1.73     | 0            |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3840  | 612    | 190.08 | 4        | 0.82         |

_Table 1. Detailed evaluation of Gemini Experimental in EPAM's LLMs Benchmark._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Total Output | Total Time (sec) | Token per Second* | Experiment cost ($) | Average Accuracy | Average Completeness |
|-------------|---------------|--------------|------------------|-------------------|---------------------|------------------|----------------------|
| 87809       | 0             | 67061        | 2091.58          | 32.06             | -                   | 3.517            | 3.197                |

_Table 2. Summary of Gemini Experimental performance in EPAM's LLMs Benchmark._

This summary shows that Gemini Experimental has a strong average accuracy of 3.517 and a solid average completeness of 3.197 across all experiments. The model processed a total of 87,809 input tokens, generating 67,061 output tokens in
2091.58 seconds, resulting in an average speed of 32.06 tokens per second.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time    | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|-----------------|----------------|-----------------------|
| Code Translation   | 3.861            | 3.860                | 95.260 seconds  | 4075.286       | 42.781                |
| Code Generation    | 3.289            | 2.957                | 61.595 seconds  | 2612.571       | 42.415                |
| Code Documentation | 3.428            | 2.869                | 110.399 seconds | 2249.556       | 20.377                |

_Table 3. Detailed analysis of Gemini Experimental performance in EPAM's LLMs Benchmark._

- Code Translation: The model demonstrated exceptional performance in code translation tasks, achieving high scores in both accuracy (3.861) and completeness (3.860). The average processing time was 95.260 seconds, with a respectable speed
  of 42.781 tokens per second.
- Code Generation: Performance in code generation tasks was strong, with an average accuracy of 3.289 and completeness of 2.957. The model was particularly efficient in these tasks, with an average time of 61.595 seconds and maintaining a
  similar token processing speed of 42.415 tokens per second.
- Code Documentation: The model showed good capabilities in documentation tasks, scoring 3.428 in accuracy and 2.869 in completeness. Documentation tasks took longer on average at 110.399 seconds, with a lower processing speed of 20.377
  tokens per second.

Large Context Instruction Following (LCIF) score: 100%

| Experiment       | Accuracy | Completeness | Time        | Input Tokens | Output Tokens |
|------------------|----------|--------------|-------------|--------------|---------------|
| TranslateToReact | 4        | 4            | 221 seconds | 75533        | 3837          |
| UpdateReact      | 4        | 4            | 192 seconds | 79668        | 3770          |

_Table 4. Gemini Experimental performance in Large Context Instruction Following (LCIF) tasks._

LCIF experiment notes: Gemini Experimental demonstrated perfect instruction following capabilities, achieving a flawless score in both experiments with no errors.

1. TranslateToReact: The model successfully translated a large React project, processing 75,533 input tokens and generating 3,837 output tokens in 221 seconds.
2. UpdateReact: The model effectively updated a React project, handling 79,668 input tokens and producing 3,770 output tokens in 192 seconds.

## Conclusion

The comprehensive evaluation of Gemini Experimental reveals a highly capable model with particularly strong performance in certain areas. Here are the key findings:

1. Performance Across Categories:
    - Outstanding in Code Translation (accuracy: 3.861, completeness: 3.860)
    - Strong in Code Generation (accuracy: 3.289, completeness: 2.957)
    - Solid performance in Code Documentation (accuracy: 3.428, completeness: 2.869)
    - Perfect LCIF score (100%), demonstrating exceptional ability to handle large contexts

2. Operational Characteristics:
    - Varied processing speeds across different tasks:
        * Code Translation: 42.781 tokens/second
        * Code Generation: 42.415 tokens/second
        * Code Documentation: 20.377 tokens/second
    - Longer average processing times compared to some competitors:
        * Code Translation: 95.260 seconds
        * Code Generation: 61.595 seconds
        * Code Documentation: 110.399 seconds

3. Strengths:
    - Perfect score in LCIF tasks (100%)
    - Exceptional accuracy in code translation tasks
    - Consistent performance across different framework translations
    - High accuracy scores across all categories (>3.2)
    - Particularly strong in handling complex framework migrations

4. Areas for Improvement:
    - Processing speed could be optimized, particularly for documentation tasks
    - Completeness scores in documentation tasks (2.869) lag behind accuracy scores (3.428)
    - Higher average processing times compared to some competing models
    - Variable performance in test generation for legacy code

Overall, Gemini Experimental positions itself as a top-tier model in the current LLM landscape, particularly excelling in code translation and large context handling. Its perfect LCIF score and high accuracy across all categories
demonstrate its reliability for complex coding tasks. While processing speeds may not lead the field, the model's consistent high-quality output and perfect instruction following capabilities make it a strong choice for professional
development tasks.

The model's exceptional performance in framework translations and perfect LCIF score suggest it's particularly well-suited for large-scale code migration projects and complex software development tasks where accuracy and completeness are
prioritized over processing speed.


<p style="text-align: center;">
    Â© 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>