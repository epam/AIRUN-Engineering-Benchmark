# Grok 2 (2024-12-19)

## Introduction

This report provides a detailed evaluation of the Grok 2 language model's performance on various code-related tasks. The model was tested on tasks involving code translation, generation, and documentation across different programming
languages and datasets. The analysis includes an overview of the experiments conducted, the categories evaluated, and the model's accuracy and completeness scores.

## Evaluation Summary

The following table summarizes the detailed performance of the Grok 2 model across all experiments:

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Output | Time  | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|--------|-------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3285  | 3009   | 49.00 | 4        | 3            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2379  | 2740   | 45.40 | 4        | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5381  | 3850   | 58.60 | 4        | 4            |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3271  | 2689   | 41.65 | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2313  | 2901   | 51.45 | 3.22     | 3.04         |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1251  | 1578   | 24.11 | 3.85     | 3.01         |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1365  | 3210   | 50.25 | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 372   | 610    | 10.00 | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 196   | 1403   | 24.77 | 4        | 4            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 185   | 699    | 10.84 | 3.01     | 4            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1203  | 1904   | 28.80 | 3.01     | 3.88         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3237  | 2873   | 45.88 | 4        | 2.98         |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5355  | 2271   | 36.82 | 1.5      | 1.03         |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 15757 | 1795   | 30.40 | 4        | 3.38         |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1193  | 942    | 16.30 | 4        | 3.88         |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3227  | 1030   | 16.46 | 4        | 4            |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5345  | 1028   | 16.49 | 3.1      | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1316  | 2555   | 38.97 | 3.44     | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3350  | 2261   | 36.62 | 4        | 3.5          |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5468  | 1820   | 28.03 | 1.99     | 1.03         |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5426  | 1420   | 22.08 | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1274  | 903    | 13.99 | 4        | 2.71         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3308  | 1526   | 23.25 | 4        | 4            |

_Table 1. Detailed evaluation of Grok 2 in EPAM's LLMs Benchmark._

Below is a summary of the overall performance across all experiments:

| Total Input | Total Reasons | Total Output | Total Time (sec) | Token per Second* | Experiment cost ($) | Average Accuracy | Average Completeness | 
|-------------|---------------|--------------|------------------|-------------------|---------------------|------------------|----------------------|
| 75457       | 0             | 45017        | 720.120          | 62.52             | 0.60                | 3.61             | 3.45                 |

_Table 2. Summary of Grok 2 performance in EPAM's LLMs Benchmark._

This summary shows that Grok 2 achieved a strong average accuracy of 3.61 and average completeness of 3.45 across all experiments. The model processed a total of 75,457 input tokens, generating 45,017 output tokens in 720.12 seconds,
demonstrating an efficient processing speed of 62.52 tokens per second. Notably, the total experiment cost was only $0.60, indicating good cost efficiency.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 3.867            | 3.579                | 45.779 seconds | 2853.857       | 62.340                |
| Code Generation    | 3.360            | 3.324                | 26.784 seconds | 1650.714       | 61.630                |
| Code Documentation | 3.614            | 3.458                | 23.575 seconds | 1498.333       | 63.556                |

_Table 3. Detailed analysis of Grok 2 performance in EPAM's LLMs Benchmark._

- Code Translation: The model demonstrated excellent performance in code translation tasks, achieving a high average accuracy of 3.867 and completeness of 3.579. The average processing time was 45.779 seconds, maintaining a steady speed of
  62.34 tokens per second.
- Code Generation: The model showed strong capabilities in code generation with an average accuracy of 3.360 and completeness of 3.324. It processed tasks efficiently with an average time of 26.784 seconds at 61.63 tokens per second.
- Code Documentation: The model performed very well in documentation tasks, with a high average accuracy of 3.614 and completeness of 3.458. It showed particularly efficient processing in this category, averaging 23.575 seconds per task at
  63.556 tokens per second.

Large Context Instruction Following (LCIF) score: 82.25%

| Experiment       | Accuracy | Completeness | Time       | Input Tokens | Output Tokens |
|------------------|----------|--------------|------------|--------------|---------------|
| TranslateToReact | 3        | 3            | 72 seconds | 61952        | 3492          |
| UpdateReact      | 4        | 3            | 77 seconds | 61981        | 3874          |

_Table 4. Grok 2 performance in Large Context Instruction Following (LCIF) tasks._

LCIF experiment notes: Grok 2 showed strong performance in handling large context tasks, achieving consistent scores across both translation and update scenarios. The model maintained steady performance even with input sizes exceeding
60,000 tokens.

## Conclusion

The comprehensive evaluation of Grok 2 reveals a highly capable model with strong performance across all tested categories. Here are the key findings:

1. Performance Across Categories:
    - Exceptional in Code Translation (accuracy: 3.867, completeness: 3.579)
    - Strong in Code Documentation (accuracy: 3.614, completeness: 3.458)
    - Reliable in Code Generation (accuracy: 3.360, completeness: 3.324)
    - Robust LCIF score of 82.25%, showing strong capability with large contexts

2. Operational Efficiency:
    - Consistent processing speed across all categories (61-63 tokens per second)
    - Efficient task completion times:
        * Code Translation: 45.8 seconds average
        * Code Generation: 26.8 seconds average
        * Code Documentation: 23.6 seconds average
    - Excellent handling of large contexts (processing 60,000+ tokens in 72-77 seconds)

3. Strengths:
    - High accuracy across all categories, particularly in code translation
    - Consistent performance with both small and large inputs
    - Cost-effective operation ($0.60 for the entire test suite)
    - Very balanced completeness scores across categories
    - Particularly strong with React-based tasks

4. Areas for Improvement:
    - Slight variation in performance with Angular-based tasks
    - Some inconsistency in test generation for complex codebases
    - Lower completeness scores in certain edge cases with high-complexity tasks

Overall, Grok 2 demonstrates significant improvements over its beta version, positioning itself as a highly competitive model in the current LLM landscape. Its balanced performance across all categories, coupled with consistent processing
speeds and strong accuracy scores, makes it a reliable choice for a wide range of coding tasks. The model's capability to handle both small and large contexts efficiently, while maintaining high accuracy, suggests a robust and
well-optimized architecture.

The model's performance places it among the top-tier competitors, particularly impressive given its cost efficiency. Future iterations could focus on improving consistency with Angular framework tasks and enhancing test generation
capabilities for complex codebases to further strengthen its position in the market.


<p style="text-align: center;">
    Â© 2024 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>