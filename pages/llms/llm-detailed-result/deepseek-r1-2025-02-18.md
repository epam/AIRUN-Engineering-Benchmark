# DeepSeek R1 (2025-02-18)

## Introduction

This article presents an analysis of various code-related tasks performed by the DeepSeek R1 language model, an open-source model designed for self-hosted deployments. We'll examine the model's performance across different categories of
code manipulation, including translation, generation, and documentation, while also considering its unique characteristics as a self-hosted solution.

> **Note on DeepSeek API usage**  
> Due to the fact that at the moment there is no way to use DeepSeek through their platform for a lower price, we used DeepSeek through the Fireworks API with the price $3/$8 for input/output tokens.

## Evaluation Summary

| Experiment         | Category                        | Language | Dataset           | Complexity | Size  | Input | Output | Time        | Accuracy | Completeness |
|--------------------|---------------------------------|----------|-------------------|------------|-------|-------|--------|-------------|----------|--------------|
| code_translation   | ReactToAngular                  | React    | ToDoApp_ReactJS   | high       | avg   | 3355  | 4037   | 86.06803799 | 4        | 4            |
| code_translation   | jQueryToReact                   | jQuery   | ToDoApp_jQuery    | high       | low   | 2423  | 3663   | 74.24575996 | 4        | 4            |
| code_translation   | AngularToReact                  | Angular  | AngularCosmoPage  | avg        | high  | 5630  | 3142   | 50.12481689 | 4        | 3            |
| code_translation   | UpdateReact                     | React    | ToDoApp_ReactJS   | high       | avg   | 3345  | 6549   | 100.0037971 | 4        | 4            |
| code_translation   | UpdateAngular                   | Angular  | ToDoApp_AngularJS | avg        | avg_2 | 2416  | 2975   | 54.46803999 | 3.15     | 4            |
| code_translation   | ReactToAngular                  | React    | ReactSignUp       | high       | low   | 1285  | 2465   | 31.9834981  | 3        | 3            |
| code_translation   | VanillaToReact                  | NativeJS | Piano_NativeJS    | high       | low   | 1423  | 7295   | 143.283654  | 4        | 4            |
| code_generation    | ModifyReactApp                  | React    | ReactFetchAPI     | avg        | low   | 376   | 3878   | 90.88324308 | 4        | 4            |
| code_generation    | GenerateBaseComponent           | none     | none              | none       | none  | 192   | 1787   | 50.09490299 | 4        | 4            |
| code_generation    | GenerateReactApp                | none     | none              | none       | none  | 183   | 1375   | 26.8595221  | 2.98     | 4            |
| code_generation    | WriteTestsForLegacyCode         | React    | ReactSignUp       | high       | low   | 1242  | 7182   | 109.4751189 | 4        | 3.94         |
| code_generation    | WriteTestsForLegacyCode         | React    | ToDoApp_ReactJS   | high       | avg   | 3312  | 3651   | 90.93164086 | 4        | 2.33         |
| code_generation    | WriteTestsForLegacyCode         | Angular  | AngularCosmoPage  | avg        | high  | 5605  | 2817   | 80.65044093 | 1.86     | 3            |
| code_generation    | WriteTestsForActualCode         | React    | ReactSelect       | extra_high | high  | 16276 | 2641   | 75.77174616 | 4        | 4            |
| code_documentation | BusinessFunctionality           | React    | ReactSignUp       | high       | low   | 1232  | 1271   | 17.41967368 | 4        | 4            |
| code_documentation | BusinessFunctionality           | React    | ToDoApp_ReactJS   | high       | avg   | 3302  | 1511   | 24.88188171 | 4        | 4            |
| code_documentation | BusinessFunctionality           | Angular  | AngularCosmoPage  | avg        | high  | 5595  | 1060   | 17.45829368 | 4        | 4            |
| code_documentation | EvaluateCodeQuality             | React    | ReactSignUp       | high       | low   | 1358  | 1854   | 23.62836885 | 4        | 2.99         |
| code_documentation | EvaluateCodeQuality             | React    | ToDoApp_ReactJS   | high       | avg   | 3428  | 4517   | 69.5761261  | 3        | 4            |
| code_documentation | EvaluateCodeQuality             | Angular  | AngularCosmoPage  | avg        | high  | 5721  | 2013   | 30.56115198 | 4        | 4            |
| code_documentation | DescribeTechnicalImplementation | Angular  | AngularCosmoPage  | avg        | high  | 5674  | 1495   | 22.17642665 | 4        | 1.99         |
| code_documentation | DescribeTechnicalImplementation | React    | ReactSignUp       | high       | low   | 1311  | 1580   | 52.79176497 | 4        | 2.78         |
| code_documentation | DescribeTechnicalImplementation | React    | ToDoApp_ReactJS   | high       | avg   | 3381  | 2142   | 58.05760622 | 4        | 4            |

_Table 1. Detailed evaluation of DeepSeek R1 in EPAM's LLMs Benchmark._

Below is a summary of the overall performance across all experiments:

| Total Input | Output | Total Time (sec) | Token per Second | Experiment cost ($) | Average Accuracy | Average Completeness |
|-------------|--------|------------------|------------------|---------------------|------------------|----------------------|
| 78065       | 70900  | 3527.58          | 51.32            | 0.80                | 3.74             | 3.61                 |

_Table 2. Summary of DeepSeek R1 performance in EPAM's LLMs Benchmark._

The summary reveals that DeepSeek R1 demonstrates solid performance across all evaluation categories, achieving an average accuracy of 3.74 and completeness of 3.61. While its token processing speed of 51.32 tokens per second is relatively
low compared to cloud-based solutions, it maintains consistent quality in its outputs. The total experiment cost of $0.80 through the Fireworks API suggests reasonable cost-effectiveness for production usage.

## Detailed Analysis

| Experiment         | Average Accuracy | Average Completeness | Average Time   | Average Tokens | Average Tokens/second |
|--------------------|------------------|----------------------|----------------|----------------|-----------------------|
| Code Translation   | 3.736            | 3.714                | 77.168 seconds | 4303.714       | 55.771                |
| Code Generation    | 3.549            | 3.610                | 74.952 seconds | 3333.000       | 44.468                |
| Code Documentation | 3.889            | 3.529                | 35.172 seconds | 1938.111       | 55.103                |

_Table 3. Detailed analysis of DeepSeek R1 performance by categories in EPAM's LLMs Benchmark._

The detailed analysis reveals several interesting patterns in DeepSeek R1's performance:

- Code Translation Performance:
    - Shows strong capabilities with 3.736 accuracy and 3.714 completeness
    - Particularly effective in framework translations, with perfect scores in several React-related tasks
    - Processing speed averages 55.771 tokens per second for translation tasks
    - Demonstrates consistent performance across different complexity levels

- Code Generation Capabilities:
    - Maintains good performance with 3.549 accuracy and 3.610 completeness
    - Excels in basic component generation and app modifications
    - Shows some variability in test writing tasks, ranging from 1.86 to 4.0
    - Processing speed slightly lower at 44.468 tokens per second

- Code Documentation Efficiency:
    - Achieves highest accuracy in documentation tasks (3.889)
    - Strong performance in business functionality documentation
    - Maintains consistent completeness (3.529) across different documentation types
    - Processes documentation tasks efficiently at 55.103 tokens per second

Large Context Instruction Following (LCIF) score: 78.13%

| Experiment       | Accuracy | Completeness | Time        | Input Tokens | Output Tokens |
|------------------|----------|--------------|-------------|--------------|---------------|
| TranslateToReact | 3.5      | 3            | 645 seconds | 127400       | 21066         |
| UpdateReact      | 3.5      | 2.5          | 621 seconds | 80311        | 21426         |

_Table 4. DeepSeek R1 performance in Large Context Instruction Following (LCIF) tasks._

In LCIF tasks, DeepSeek R1 achieves score of 78.13%, demonstrating its ability to handle large contexts, though with some limitations. The model shows consistent accuracy (3.5) across both test cases but varies in
completeness (3.0 and 2.5). The notably long processing times (645 and 621 seconds) for large contexts indicate a need for optimization in handling extensive inputs.
Also, worth noting is that the model was difficult to make follow instructions, by fine-tuning the prompt it was possible to get more or less adequate results, but still the model did not always follow the instructions.

## Conclusion

DeepSeek R1 demonstrates several notable characteristics and capabilities that position it as a promising option for organizations considering self-hosted LLM solutions:

1. Performance Strengths:
    - Achieves competitive scores across all categories (84.0% total benchmark score), one of the best from open-source models
    - Particularly strong in code documentation (87.3%) and code generation (83.9%)
    - Maintains consistent quality in framework translations (86.8%)

2. Deployment Advantages:
    - Offers a viable self-hosted alternative to cloud-based solutions
    - Provides full control over data and deployment environment
    - Competitive performance compared to other open-source alternatives

3. Considerations:
    - Processing speed (51.32 tokens/second) is significantly lower than cloud-based alternatives
    - LCIF performance (78.1%) indicates room for improvement in handling large contexts
    - Resource requirements need to be considered for self-hosted deployment

4. Use Case Recommendations:
    - Well-suited for organizations requiring data privacy and control
    - Appropriate for development environments with moderate latency requirements
    - Effective for standard code-related tasks across multiple frameworks
    - Good for one shot prompting, but not for complex instructions

<p style="text-align: center;">
    © 2025 EPAM Systems, Inc. All Rights Reserved.<br/>
    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>
    This report is licensed under CC BY-SA 4.0<br/>
</p>