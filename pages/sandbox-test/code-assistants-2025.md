# Code Assistants Leaderboard as of April 2025

- [Leaderboard as of December 2024](code-assistants.md)
- [Leaderboard per Developer Task as of April 2025](#leaderboard-per-developer-task-as-of-april-2025)

## Leaderboard as of April 2025

| Category                                                                                                                   | Test Date  | Test Details                                                                                                                                                                                                         | Executive Summary on Code Assistant Research                                                                                                                                                                                                                         | Final Score (Chat Based + Code Completion Tests) | Chat Based Tests (109 tests) | Code Completion Tests (60 tests) |
|----------------------------------------------------------------------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|------------------------------|----------------------------------|
| [GitHub Copilot](https://github.com/features/copilot) + [GPT-4o](https://platform.openai.com/docs/models/gpt-4o)           | March 2025 | [Sandbox Tests - March 2025](reports/copilot/2025/copilot-gpt4o-sandbox-tests-march-2025.md) <br> [Golf App Tests - March 2025](reports/copilot/2025/copilot-gpt4o-golf-app-tests-march-2025.md)                     | Appeared to be the best in code-completions and very good at chat tests (but not the best). As of Spring 25 has agentic feature in very heavy development/experimental state. Lacking some enterprise features like standalone deployment. Improved plugins for JetBrains tools and full Visual Studio - they almost caught on VS Code. | **88%**                                         | 91%                          | **82%**                         |
| [Cursor](https://www.cursor.com/) + [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3.7-sonnet)                  | March 2025 | [Sandbox Tests - March 2025](reports/cursor/2025/cursor-sonnet3.7-sandbox-tests-march-2025.md) <br> [Golf App Tests - March 2025](reports/cursor/2025/cursor-sonnet3.7-golf-app-tests-march-2025.md)                 | A landmark of how standalone GenAI-enabled IDE may look like in the future. VS Code-based only - which may pose significant limitations for enterprise developers (Java, .NET, front-end) - in such cases may represent an interface to AI features including the agent. Has the most capable agentic feature, although this feature isn't ideal - can solve small to medium tasks with moderate complexity. | 87%                                             | **95%**                      | 72%                             |
| [Cursor](https://www.cursor.com/) + [Gemini-2.5-pro-exp-03-25](https://cloud.google.com/gemini/docs/overview)              | March 2025 | [Sandbox Tests - March 2025](reports/cursor/2025/cursor-gemini2.5-sandbox-tests-march-2025.md) <br> [Golf App Tests - April 2025](reports/cursor/2025/cursor-gemini2.5-golf-app-tests-april-2025.md)                 | A landmark of how standalone GenAI-enabled IDE may look like in the future. VS Code-based only - which may pose significant limitations for enterprise developers (Java, .NET, front-end) - in such cases may represent an interface to AI features including the agent. Has the most capable agentic feature, although this feature isn't ideal - can solve small to medium tasks with moderate complexity. | 87%                                             | **95%**                      | 72%                             |
| [Gemini](https://gemini.google.com/app)                                                                                    | April 2025 | [Sandbox Tests - April 2025](reports/gemini/2025/gemini-sandbox-tests-april-2025.md) <br> [Golf App Tests - April 2025](reports/gemini/2025/gemini-golf-app-tests-april-2025.md)                                     | Good quality and speed, but Gemini by Google does not support multi-model selection or agentic flow capabilities (agentic was added on the day of completing the benchmark run). Compared to Amazon Q, Windsurf (Claude 3.7 Sonnet), and Continue (Claude 3.7 Sonnet), Gemini generated responses in the chat window at least twice as fast. | 83%                                             | 90%                          | 70%                             |
| [Cursor](https://www.cursor.com/) + [GPT-4o](https://platform.openai.com/docs/models/gpt-4o)                               | March 2025 | [Sandbox Tests - March 2025](reports/cursor/2025/cursor-gpt4o-sandbox-tests-march-2025.md) <br> [Golf App Tests - April 2025](reports/cursor/2025/cursor-gpt4o-golf-app-tests-april-2025.md)                         | A landmark of how standalone GenAI-enabled IDE may look like in the future. VS Code-based only - which may pose significant limitations for enterprise developers (Java, .NET, front-end) - in such cases may represent an interface to AI features including the agent. Has the most capable agentic feature, although this feature isn't ideal - can solve small to medium tasks with moderate complexity. | 82%                                             | 88%                          | 72%                             |
| [Amazon Q](https://aws.amazon.com/q/)                                                                                      | April 2025 | [Sandbox Tests - April 2025](reports/amazon-q/2025/amazon-q-sandbox-tests-april-2025.md) <br> [Golf App Tests - April 2025](reports/amazon-q/2025/amazon-q-sandbox-tests-april-2025.md)                              | Amazon Q, developed by AWS, demonstrates good performance and quality (although not the best) and was one of the first coding assistants to provide agentic flow capabilities. However, it does not support multi-model selection.                                                                  | 82%                                             | 91%                          | 65%                             |
| [Cody (Sourcegraph)](https://sourcegraph.com/cody) + [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3.7-sonnet) | March 2025 | [Sandbox Tests - March 2025](reports/sourcegraph-cody/2025/cody-sonnet3.7-sandbox-tests-march-2025.md) <br> [Golf App Tests - March 2025](reports/sourcegraph-cody/2025/cody-sonnet3.7-golf-app-tests-march-2025.md) | Quality is good - a bit worse than leaders but almost on par with Cursor + GPT-4o. Provides very wide spectrum of enterprise features like standalone deployment, wide support of IDE plugins, prompt library, code insights, code search etc. Agentic interface is in heavy development but already is of moderate usefulness/quality.                    | 80%                                             | 90%                          | 63%                             |
| [Windsurf](https://windsurf.com/editor)                                                                                    | April 2025 | [Sandbox Tests - April 2025](reports/windsurf/2025/windsurf-sonnet3.7-sandbox-tests-april-2025.md) <br> [Golf App Tests - April 2025](reports/windsurf/2025/windsurf-sonnet3.7-golf-app-tests-april-2025.md)         | Windsurf, a standalone AI assistant, has so far demonstrated lower performance compared to other tools, however it has a powerful agentic feature (not included into this benchmark run). Windsurf performs worse on code completion tests, but it is good at solving chat-based tests. Also, additional testing demonstrated its full code indexing capability. | 76%                                             | 91%                          | 50%                             |

<div style='text-align: right;'> © EPAM. Distribution and use permitted </div>

- All the tests have been performed in VS Code or IntelliJ IDEA IDEs except Cursor and Windsurf that represent their own IDE. In other IDEs score and behavior of code assistants may vary.
- The LLM specified for the code assistant was used to run tests for the chat window.

## Leaderboard per Developer Task as of April 2025
![code-assistants-performance-dec-2024.png](../../images/sandbox-test/code-assistants-performance-engineering-benchmark-detailed-apr-25-2.png)
<div style='text-align: right;'> © EPAM. Distribution and use permitted </div>

<p style="text-align: center;">    © 2025 EPAM Systems, Inc. All Rights Reserved.<br/>    EPAM, EPAM AI/RUN <sup>TM</sup> and the EPAM logo are registered trademarks of EPAM Systems, Inc.<br>    This report is licensed under CC BY-SA 4.0<br/></p>

